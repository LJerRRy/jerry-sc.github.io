---
layout: post
title: 对偶支持向量机
date: 2016-12-08
author: Jerry
header-img: img/blue.png
catalog: true
tags:
- Machine Learning
---

### 问题引入

![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/1.png)

上文学习了硬间隔支持向量机，其只适合用于线性可分情况下，其分隔线为一个超平面。
当然我们可以将原来特征进行非线性转换，使得SVM求解出一个非线性复杂分割面。利用SVM最大化间隔的特性，可以找到一个不错结果。然而到延伸到高维空间中去的时候，权重向量w的复杂度也不断增加，使得问题的求解变得复杂。因此，引入原SVM的对偶问题，通过求解对偶问题的解间接得到原问题的解；而对偶问题的复杂度并不会随空间维度升高而增加。

### 对偶形式

![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/2.png)

通常，带有约束条件的最优问题较难处理，我们通过引进拉格朗日乘子，构造拉格朗日函数。进一步转化得SVM的另一种等价形式：
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/3.png)

此时，约束条件隐藏于求解max中，在最后求解关于w，b的min过程中，无需考虑约束条件。进一步转化，将最大最小顺序变化，得到原始问题的对偶形式：
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/4.png)

![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/5.png)

如上所示，当且仅当二次规划满足
- 凸函数
- 存在可行性解
- 线性约束

对偶问题与原始问题最终值相同（并不代表w,b相同，只是最终结果相同，后文会继续介绍KTT）。显然，原始问题全部满足这三个条件。

### 对偶问题的求解
将对偶问题展开有：
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/6.png)

因此当我们在计算关于w,b的最小化问题时，无需考虑alpha的影响。

![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/7.png)

由梯度下降可知，当我们取得最优解的时候，目标函数对w,b的偏导数为0。
首先来看对b的偏导数为0，得到上述约束等式。将此式移到最外面，并不影响最终结果，此外这样带来的好处是可以将内部化简，消去b。
同理，我们队w进行相同操作，化简后得：
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/8.png)

两步操作以后，我们就可以消去min动作，消去w，b。
由于上述操作均满足KTT最优条件，所以通过求解对偶问题得到的w,b同样也是原问题的解。
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/9.png)

将上述问题变形有：
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/10.png)

由于在求解关于alpha的最小化问题，所以关于w的求解等式可以先忽略。同样这也是一个二次规划问题，只不过在这里，二次规划的变量个数为N，而不是w的复杂度，从而有效解决了非线性转换带来的高维度问题。
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/11.png)

![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/12.png)

通常情况下，Q矩阵中都为非0元素，因此在进行计算过程中会占用非常大内存，实际求解过程中，我们需要采取特殊的二次规划求解方式，从而加快计算。
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/13.png)

当计算出alpha值时，根据之前的公式，就可以计算得到w。
根据KTT中的最后一条互补公式，如果alpha值大于0，就可根据等式计算得到b。回顾之前最初SVM的定义可以发现，当alpha大于0时，这些样本点恰好是SV支持向量，落在边界上。
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/14.png)

### 对偶问题的隐含信息
再来看一下w和b的计算公式可以发现，两者其实都是由SV支持向量决定的，与其他样本点无关。
所以从SVM对偶问题的求解过程中，我们可以清楚的看到，最后求得的分隔面只是由支持向量决定。

![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/15.png)

更进一步，可以发现w其实由yz的线性组合构成，这其实和PLA中w的计算相似，只不过在PLA中，系数为分类错误次数决定。
更一般地，线性回归，逻辑回归，都可以看做是由yz的线性组合。我们把这样的w称为“由数据表示的w”；在SVM中，w只不过是由SV支持向量决定而已。
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/16.png)

以上是SVM原始问题与对偶问题的比较，当样本数比较小的时候，使用对偶问题比较合适；当w的复杂度不是很高的时候，使用原始问题比较合适。
![enter image description here](http://7xt1xs.com1.z0.glb.clouddn.com/ml/dual_svm/17.png)

然而，虽然从对偶问题的式子中，好像对偶问题只依赖于样本数，而与w的复杂度无关。然而，实际上在计算Q矩阵中，仍然依赖于w复杂度，这也是我们之后需要进一步解决的问题。

### Reference
- 台大《机器学习》——林轩田